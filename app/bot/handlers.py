import inspect
import time
import asyncio
import logging
import json
# (same as the "updated_ocr_handlers.zip" version I prepared earlier)
# Included here in full for convenience.
import os, re, time, asyncio, logging, difflib, json
from pathlib import Path
import zipfile
import tempfile
from typing import List, Tuple, Dict, Optional, Any, Union

# --- ensure logging shows our INFO messages ---
_semantic_logger = logging.getLogger("semantic-bot")
if not _semantic_logger.handlers:
    _handler = logging.StreamHandler()
    _formatter = logging.Formatter("[%(levelname)s] %(name)s: %(message)s")
    _handler.setFormatter(_formatter)
    _semantic_logger.addHandler(_handler)
_semantic_logger.setLevel(logging.INFO)
del _handler, _formatter



from telegram import Update, Document, ReplyKeyboardMarkup, InputFile
from telegram.ext import MessageHandler, CommandHandler, filters, ApplicationBuilder

from app.config import (
    TELEGRAM_BOT_TOKEN, DOC_FOLDER, INDEX_FILE, TEXTS_FILE, MANIFEST_FILE,
    DAILY_FREE_LIMIT, TELEGRAM_MSG_LIMIT, RUN_MODE, PUBLIC_BASE_URL, RETRIEVAL_K,
    FORCE_OCR
)
from app.utils.usage import is_admin, get_usage, inc_usage
from app.utils.files import load_manifest, save_manifest, sha256_file
from app.services.indexing import index_file, retrieve_chunks
from app.services.generation import generate_direct_ai_answer, generate_answer_with_gemini
from app.services.tm import tm_process_search, ROW_MATCH_EXPERTISE, ROW_MATCH_REGISTERED, ROW_MATCH_KW
from app.services.extract import extract_text_from_pdf, extract_text_from_docx, extract_text_from_txt, extract_text_from_doc
from app.services.chunking import smart_split_text
from app.services.embeddings import get_embedding
import io
import tempfile
from app.ocr.postprocess import postprocess

from docx import Document as DocxDocument
from docx.enum.text import WD_COLOR_INDEX

import fitz  # PyMuPDF
from pdf2image import convert_from_path
import pytesseract

logger = logging.getLogger("semantic-bot")

AI_LABEL   = "ü§ñ AI-—á–∞—Ç"
DOCS_LABEL = "FAQ"
WORK_LABEL = "üóÇÔ∏è –†–∞–±–æ—Ç–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"
TM_LABEL   = "üè∑Ô∏è –¢–æ–≤–∞—Ä–Ω—ã–µ –∑–Ω–∞–∫–∏"
MAIN_KB    = ReplyKeyboardMarkup([[AI_LABEL, WORK_LABEL, TM_LABEL], [DOCS_LABEL]], resize_keyboard=True)

TM_MODE = "tm"

# ---- OCR defaults tuned for Russian docs ----
OCR_DPI = int(os.getenv("OCR_DPI", "300"))
PDF_COMPRESS = os.getenv("PDF_COMPRESS", "1") == "1"
OCR_MAX_PAGES = int(os.getenv("OCR_MAX_PAGES", "0"))  # 0 = all
POPPLER_PATH = os.getenv("POPPLER_PATH", "").strip() or None
TESS_LANG = os.getenv("TESS_LANG", "rus")  # —á–∏—Å—Ç–æ —Ä—É—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
TESS_CONFIG = os.getenv("TESS_CONFIG", "--oem 3 --psm 6 -c preserve_interword_spaces=1")

# Dedup windows
PDF_DEDUP_TTL = 120
INDEX_DEDUP_TTL = 600

def _split_for_telegram(text: str, max_len: int = TELEGRAM_MSG_LIMIT - 200) -> list[str]:
    parts, buf, cur = [], [], 0
    for p in text.replace("\r\n", "\n").split("\n\n"):
        p = p.strip()
        if not p:
            chunk = "\n\n".join(buf).strip()
            if chunk: parts.append(chunk)
            buf, cur = [], 0
            continue
        need = len(p) + (2 if cur > 0 else 0)
        if cur + need <= max_len:
            buf.append(p); cur += need
        else:
            chunk = "\n\n".join(buf).strip()
            if chunk: parts.append(chunk)
            buf, cur = [], 0
            while len(p) > max_len:
                parts.append(p[:max_len]); p = p[max_len:]
            if p:
                buf, cur = [p], len(p)
    chunk = "\n\n".join(buf).strip()
    if chunk: parts.append(chunk)
    return parts



async def _send_file_safely(update: Update, path: str, download_name: str, caption: str = ""):
    """Send file with size check; auto-zip if too large for Telegram bot (‚âà50MB limit)."""
    try:
        if not os.path.exists(path):
            await update.message.reply_text("–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return
        size = os.path.getsize(path)
        # Telegram –±–æ—Ç–∞–º –ª—É—á—à–µ –¥–µ—Ä–∂–∞—Ç—å—Å—è < 50 MB
        limit = 45 * 1024 * 1024
        if size > limit:
            # zip on the fly into temp dir
            with tempfile.TemporaryDirectory() as td:
                zip_path = os.path.join(td, download_name + ".zip") if not download_name.endswith(".zip") else os.path.join(td, download_name)
                with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as z:
                    z.write(path, arcname=download_name)
                with open(zip_path, "rb") as f:
                    await update.message.reply_document(document=InputFile(f, filename=os.path.basename(zip_path)),
                                                        caption=(caption + " (—É–ø–∞–∫–æ–≤–∞–Ω–æ –≤ ZIP)")[:1024])
        else:
            with open(path, "rb") as f:
                await update.message.reply_document(document=InputFile(f, filename=download_name), caption=caption[:1024])
    except Exception as e:
        await update.message.reply_text(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ñ–∞–π–ª–∞: {e}")

async def send_long(update: Update, text: str):
    text = (text or "").strip() or "‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∑–∞–Ω–æ–≤–æ."
    for c in _split_for_telegram(text):
        await update.message.reply_text(c, disable_web_page_preview=True)

def _normalize_ext(ext: str) -> str:
    mapping = {"—Å":"c","—Ö":"x","–æ":"o","—Ä":"p","–∞":"a","–µ":"e","–∫":"k","–º":"m","—Ç":"t","–Ω":"h","–≤":"b"}
    return "".join(mapping.get(ch, ch) for ch in ext)

# ---- Dedup helpers ----
def _pdf_job_key(document: Document) -> str:
    return f"pdf::{document.file_unique_id or document.file_id}"

def _pdf_job_is_running(context, key: str) -> bool:
    jobs = context.user_data.setdefault("pdf_jobs", {})
    rec = jobs.get(key)
    if not rec: return False
    if time.time() - rec.get("ts", 0) > PDF_DEDUP_TTL:
        jobs.pop(key, None)
        return False
    return rec.get("running", False)

def _pdf_job_start(context, key: str, progress_msg_id: int | None):
    jobs = context.user_data.setdefault("pdf_jobs", {})
    jobs[key] = {"running": True, "ts": time.time(), "msg_id": progress_msg_id}

def _pdf_job_finish(context, key: str):
    jobs = context.user_data.setdefault("pdf_jobs", {})
    jobs.pop(key, None)

def _index_job_key(file_unique_id: str | None, file_hash: str | None) -> str:
    return f"idx::{file_unique_id or ''}::{file_hash or ''}"

def _idx_is_running(context, key: str) -> bool:
    jobs = context.bot_data.setdefault("index_jobs", {})
    rec = jobs.get(key)
    if not rec: return False
    if time.time() - rec.get("ts", 0) > INDEX_DEDUP_TTL:
        jobs.pop(key, None)
        return False
    return bool(rec.get("running"))

def _idx_start(context, key: str, msg_id: int | None):
    jobs = context.bot_data.setdefault("index_jobs", {})
    jobs[key] = {"running": True, "ts": time.time(), "msg_id": msg_id}

def _idx_finish(context, key: str):
    jobs = context.bot_data.setdefault("index_jobs", {})
    jobs.pop(key, None)

# ---------- Post-OCR normalization: Latin look-alikes -> Cyrillic ----------
def _normalize_confusables_ru(text: str) -> str:
    """Replace Latin look-alikes with Cyrillic and tidy spaces/punctuation."""
    if not text:
        return text
    mapping = {
        # uppercase
        "A": "–ê", "B": "–í", "C": "–°", "E": "–ï", "H": "–ù", "K": "–ö", "M": "–ú",
        "O": "–û", "P": "–†", "T": "–¢", "X": "–•", "Y": "–£",
        # lowercase
        "a": "–∞", "c": "—Å", "e": "–µ", "o": "–æ", "p": "—Ä", "x": "—Ö", "y": "—É",
        # occasional OCR junk
        "√ë": "–ù", "√ç": "–ò", "√¨": "–∏",
    }
    out = []
    for ch in text:
        out.append(mapping.get(ch, ch))
    fixed = "".join(out)
    fixed = re.sub(r"\s+([,.:;!?])", r"\1", fixed)
    fixed = fixed.replace("‚Ä¶", "...")
    return fixed

# ---------- PDF helpers ----------
def _compress_pdf_sync(src: str) -> str:
    try:
        doc = fitz.open(src)
        out = os.path.join(DOC_FOLDER, f"compressed_{Path(src).name}")
        doc.save(out, garbage=4, deflate=True, clean=True, linear=True)
        doc.close()
        return out
    except Exception as e:
        logger.warning("PDF compress failed for %s: %r", src, e)
        return src

def _ocr_pdf_to_text_sync(pdf_path: str, dpi: int, max_pages: int, poppler_path: Optional[str], tess_lang: str) -> str:
    try:
        kwargs = {"dpi": dpi, "fmt": "png"}
        if poppler_path:
            kwargs["poppler_path"] = poppler_path
        images = convert_from_path(pdf_path, **kwargs)
        if max_pages and max_pages > 0:
            images = images[:max_pages]

        texts = []
        for img in images:
            try:
                # Try OpenCV preprocessing if available
                try:
                    import cv2
                    import numpy as np
                    arr = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)
                    arr = cv2.threshold(arr, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
                    pre_img = arr
                    ocr_text = pytesseract.image_to_string(pre_img, lang=tess_lang, config=TESS_CONFIG)
                except Exception:
                    ocr_text = pytesseract.image_to_string(img, lang=tess_lang, config=TESS_CONFIG)
                ocr_text = _normalize_confusables_ru(ocr_text)
            except Exception as e:
                logger.warning("OCR page failed: %r", e)
                ocr_text = ""
            texts.append(ocr_text)
        return "\n".join(texts).strip()
    except Exception as e:
        logger.error("OCR failed for %s: %r", pdf_path, e)
        return ""

def _save_text_as_docx(text: str, base_name: str) -> str:
    os.makedirs(DOC_FOLDER, exist_ok=True)
    safe = re.sub(r"[^A-Za-z–ê-–Ø–∞-—è0-9_.-]+", "_", base_name)[:60]
    out = os.path.join(DOC_FOLDER, f"{int(time.time())}_{safe}.docx")
    doc = DocxDocument()
    for line in (text or "").split("\n"):
        doc.add_paragraph(line)
    doc.save(out)
    return out

# ---------- DOCX helpers ----------
def _save_docx_report(filename: str, title: str, body: str) -> str:
    os.makedirs(DOC_FOLDER, exist_ok=True)
    path = os.path.join(DOC_FOLDER, filename)
    doc = DocxDocument()
    doc.add_heading(title, level=1)
    for para in (body or "").split("\n"):
        doc.add_paragraph(para)
    doc.save(path)
    return path

def _add_table_change(doc: DocxDocument, old_text: str, new_text: str, caption: str | None = None):
    if caption:
        pcap = doc.add_paragraph()
        pcap.add_run(caption).bold = True
    table = doc.add_table(rows=1, cols=2)
    table.style = 'Table Grid'
    # Left: old
    cell_old = table.rows[0].cells[0]
    old_lines = (old_text or '').splitlines() or ['(–ø—É—Å—Ç–æ)']
    for i, line in enumerate(old_lines):
        p = cell_old.paragraphs[0] if i == 0 else cell_old.add_paragraph()
        p.add_run(line)

    # Right: new (yellow)
    cell_new = table.rows[0].cells[1]
    new_lines = (new_text or '').splitlines() or ['(–ø—É—Å—Ç–æ)']
    for i, line in enumerate(new_lines):
        p = cell_new.paragraphs[0] if i == 0 else cell_new.add_paragraph()
        r = p.add_run(line)
        r.font.highlight_color = WD_COLOR_INDEX.YELLOW
    doc.add_paragraph("")

def _save_docx_compare_tables(filename: str, doc_name_a: str, doc_name_b: str, text_a: str, text_b: str) -> str:
    os.makedirs(DOC_FOLDER, exist_ok=True)
    path = os.path.join(DOC_FOLDER, filename)
    doc = DocxDocument()
    doc.add_heading(f"–ò–∑–º–µ–Ω–µ–Ω–∏—è: {doc_name_a} ‚Üí {doc_name_b}", level=1)

    a_lines = [l.rstrip() for l in (text_a or "").splitlines()]
    b_lines = [l.rstrip() for l in (text_b or "").splitlines()]
    sm = difflib.SequenceMatcher(a=a_lines, b=b_lines, autojunk=False)

    change_count = 0
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag == "equal":
            continue
        change_count += 1
        old_block = "\n".join(a_lines[i1:i2]).strip()
        new_block = "\n".join(b_lines[j1:j2]).strip()
        if tag == "replace":
            _add_table_change(doc, old_block, new_block, caption=f"–†–∞–∑–ª–∏—á–∏–µ {change_count}")
        elif tag == "insert":
            _add_table_change(doc, "", new_block, caption=f"–î–æ–±–∞–≤–ª–µ–Ω–æ {change_count}")
        elif tag == "delete":
            _add_table_change(doc, old_block, "", caption=f"–£–¥–∞–ª–µ–Ω–æ {change_count}")

    if change_count == 0:
        doc.add_paragraph("–ò–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.")
    doc.save(path)
    return path

# ---------- Local fallbacks (no LLM) ----------
def _fallback_summary(text: str) -> str:
    text = (text or "").strip()
    if not text:
        return "–î–æ–∫—É–º–µ–Ω—Ç –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω."
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    head = lines[:100]
    def grab(label_variants):
        for lv in label_variants:
            m = re.search(rf"{lv}([^\n\r]{{0,200}})", text, flags=re.IGNORECASE)
            if m:
                return m.group(0).strip()
        return ""
    parties = grab(["—Å—Ç–æ—Ä–æ–Ω—ã", "–∑–∞–∫–∞–∑—á–∏–∫", "–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å", "–∞—Ä–µ–Ω–¥–æ–¥–∞—Ç–µ–ª—å", "–∞—Ä–µ–Ω–¥–∞—Ç–æ—Ä"])
    term    = grab(["—Å—Ä–æ–∫", "–ø–µ—Ä–∏–æ–¥ –¥–µ–π—Å—Ç–≤–∏—è", "–¥–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è"])
    price   = grab(["—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ", "–æ–ø–ª–∞—Ç–∞"])
    subject = grab(["–ø—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞", "–ø—Ä–µ–¥–º–µ—Ç", "—Ü–µ–ª—å –¥–æ–≥–æ–≤–æ—Ä–∞"])

    bullets = []
    if subject: bullets.append(f"‚Ä¢ –ü—Ä–µ–¥–º–µ—Ç: {subject}")
    if parties: bullets.append(f"‚Ä¢ –°—Ç–æ—Ä–æ–Ω—ã: {parties}")
    if term:    bullets.append(f"‚Ä¢ –°—Ä–æ–∫: {term}")
    if price:   bullets.append(f"‚Ä¢ –¶–µ–Ω–∞/–æ–ø–ª–∞—Ç–∞: {price}")
    if not bullets:
        bullets.append("‚Ä¢ –ö–ª—é—á–µ–≤—ã–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ ‚Äî –≤–æ–∑–º–æ–∂–Ω–æ, –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞.")

    preview = "\n".join(head[:10])
    return "–ò–¢–û–ì:\n- –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –ø–æ–ª—è–º.\n\n" + "\n".join(bullets) + (f"\n\n–§—Ä–∞–≥–º–µ–Ω—Ç—ã:\n{preview}" if preview else "")

def _fallback_check(text: str) -> str:
    text = (text or "").lower()
    if not text:
        return "–î–æ–∫—É–º–µ–Ω—Ç –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω."
    checks = [
        ("–°—Ç–æ—Ä–æ–Ω—ã/—Ä–µ–∫–≤–∏–∑–∏—Ç—ã", all(k in text for k in ["—Ä–µ–∫–≤–∏–∑", "–ø–æ–¥–ø–∏—Å", "–∞–¥—Ä–µ—Å"]) ),
        ("–ü—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞", "–ø—Ä–µ–¥–º–µ—Ç" in text),
        ("–°—Ä–æ–∫ –∏ —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ", any(k in text for k in ["—Å—Ä–æ–∫", "—Ä–∞—Å—Ç–æ—Ä–∂", "–ø—Ä–µ–∫—Ä–∞—â"])),
        ("–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å/–Ω–µ—É—Å—Ç–æ–π–∫–∞", any(k in text for k in ["–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω", "–Ω–µ—É—Å—Ç–æ", "—à—Ç—Ä–∞—Ñ"])),
        ("–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å/–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", any(k in text for k in ["–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü", "–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω", "–ø–¥–Ω"])),
        ("–§–æ—Ä—Å-–º–∞–∂–æ—Ä", any(k in text for k in ["—Ñ–æ—Ä—Å", "–Ω–µ–ø—Ä–µ–æ–¥–æ–ª"])),
        ("–ü–æ—Ä—è–¥–æ–∫ –æ–ø–ª–∞—Ç—ã", any(k in text for k in ["–æ–ø–ª–∞—Ç", "—Å—Ç–æ–∏–º–æ—Å—Ç", "—Ü–µ–Ω–∞"])),
        ("–ü—Ä–∞–≤–∞ –Ω–∞ –†–ò–î/—Ä–µ–∑-—Ç—ã —Ä–∞–±–æ—Ç", any(k in text for k in ["–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω", "–ø—Ä–∞–≤–æ", "–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª"])),
    ]
    lines = [f"‚Ä¢ {name}: {'OK' if ok else '–ü–†–û–í–ï–†–ò–¢–¨/–û–¢–°–£–¢–°–¢–í–£–ï–¢'}" for name, ok in checks]
    return "–ü–†–û–í–ï–†–ö–ê –î–û–ì–û–í–û–†–ê (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –±–µ–∑ –ò–ò):\n" + "\n".join(lines)

# ---------- Handlers ----------
async def start(update: Update, context: Any):
    context.user_data["mode"] = "docs"
    usage_left = "‚àû" if is_admin(update.effective_user.id) else max(0, DAILY_FREE_LIMIT - get_usage(update.effective_user.id))
    msg = (
        "–ü—Ä–∏–≤–µ—Ç!\n\n"
        "1) üóÇÔ∏è –†–∞–±–æ—Ç–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ ‚Äî –∑–∞–≥—Ä—É–∑–∏—Ç–µ DOC/DOCX/PDF; –¥–ª—è PDF –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–µ–ª–∞–µ–º —Å–∂–∞—Ç–∏–µ+OCR –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ DOCX.\n"
        "2) –ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã ‚Äî –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º –∏–Ω–¥–µ–∫—Å–æ–º (PDF/DOCX/TXT).\n"
        "3) ü§ñ AI-—á–∞—Ç ‚Äî —Å–≤–æ–±–æ–¥–Ω—ã–π –¥–∏–∞–ª–æ–≥.\n"
        "4) üè∑Ô∏è –¢–æ–≤–∞—Ä–Ω—ã–µ –∑–Ω–∞–∫–∏ ‚Äî –ø–æ–∏—Å–∫ –ø–æ Google Sheets.\n\n"
        f"–°–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –ª–∏–º–∏—Ç AI-—á–∞—Ç: {usage_left} —Å–æ–æ–±—â–µ–Ω–∏–π."
    )
    await update.message.reply_text(msg, reply_markup=MAIN_KB)

async def ai_mode(update: Update, context: Any):
    context.user_data["mode"] = "ai"
    usage_left = "‚àû" if is_admin(update.effective_user.id) else max(0, DAILY_FREE_LIMIT - get_usage(update.effective_user.id))
    await update.message.reply_text(
        f"–†–µ–∂–∏–º: AI-—á–∞—Ç. –°–ø—Ä–æ—Å–∏—Ç–µ —á—Ç–æ —É–≥–æ–¥–Ω–æ. –î–æ—Å—Ç—É–ø–Ω–æ —Å–µ–≥–æ–¥–Ω—è: {usage_left}.", reply_markup=MAIN_KB
    )

async def docs_mode(update: Update, context: Any):
    context.user_data["mode"] = "docs"
    await update.message.reply_text(
        "–†–µ–∂–∏–º: —á–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. –ü—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª –∏ –∑–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å.", reply_markup=MAIN_KB
    )

async def work_mode(update: Update, context: Any):
    context.user_data["mode"] = "work"
    context.user_data.setdefault("work_docs", [])
    await update.message.reply_text(
        "–†–µ–∂–∏–º: üóÇÔ∏è –†–∞–±–æ—Ç–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.\n\n"
        "–ü—Ä–∏—à–ª–∏—Ç–µ DOC/DOCX/PDF ‚Äî –¥–æ–±–∞–≤–ª—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç. –î–ª—è PDF: —Å–∂–∞—Ç–∏–µ ‚Üí OCR ‚Üí —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ DOCX.\n"
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "‚Ä¢ /doc_summary ‚Äî —Ä–µ–∑—é–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n"
        "‚Ä¢ /doc_check ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ–≥–æ–≤–æ—Ä –Ω–∞ –æ—à–∏–±–∫–∏/—Ä–∏—Å–∫–∏\n"
        "‚Ä¢ /doc_compare ‚Äî —Å—Ä–∞–≤–Ω–∏—Ç—å –¥–≤–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞ (–¥–ª—è PDF —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è OCR-–îOCX)\n"
        "‚Ä¢ /doc_clear ‚Äî —É–¥–∞–ª–∏—Ç—å –≤—Å–µ —Ä–∞–Ω–µ–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n"
        "–õ—é–±–æ–π –≤–æ–ø—Ä–æ—Å –≤ —ç—Ç–æ–º —Ä–µ–∂–∏–º–µ ‚Äî –æ—Ç–≤–µ—Ç —Å –æ–ø–æ—Ä–æ–π –Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã.",
        reply_markup=MAIN_KB
    )

async def tm_mode(update: Update, context: Any):
    context.user_data["mode"] = TM_MODE
    await update.message.reply_text(
        "–†–µ–∂–∏–º: üè∑Ô∏è –¢–æ–≤–∞—Ä–Ω—ã–µ –∑–Ω–∞–∫–∏.\n\n"
        "–û—Ç–ø—Ä–∞–≤—å—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ/–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ‚Äî –Ω–∞–π–¥—É —Å—Ç—Ä–æ–∫–∏ –≤ Google Sheets –∏ –ø—Ä–∏—à–ª—é –∫–∞—Ä—Ç–æ—á–∫–∏.\n"
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "‚Ä¢ /tm_reg ‚Äî –∑–∞–ø–∏—Å–∏, –≥–¥–µ —Å—Ç–∞—Ç—É—Å —Å–æ–¥–µ—Ä–∂–∏—Ç ¬´—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è¬ª\n"
        "‚Ä¢ /tm_exp ‚Äî –∑–∞–ø–∏—Å–∏, –≥–¥–µ —Å—Ç–∞—Ç—É—Å —Å–æ–¥–µ—Ä–∂–∏—Ç ¬´—ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞¬ª",
        reply_markup=MAIN_KB
    )

async def tm_cmd_reg(update: Update, context: Any):
    await tm_process_search(update.effective_chat.id, ROW_MATCH_REGISTERED, context)

async def tm_cmd_exp(update: Update, context: Any):
    await tm_process_search(update.effective_chat.id, ROW_MATCH_EXPERTISE, context)

async def tm_handle_text(update: Update, context: Any):
    user_text = (update.message.text or "").strip()
    kws = re.split(r"\s+", user_text)
    await tm_process_search(update.effective_chat.id, lambda row: ROW_MATCH_KW(row, kws), context)

async def _work_handle_file(update: Update, context: Any, document: Document):
    raw = document.file_name or "file"
    fname = Path(raw).name
    base, ext = os.path.splitext(fname)
    ext = _normalize_ext(ext.lower().lstrip("."))
    if ext not in ("pdf", "docx", "doc"):
        await update.message.reply_text("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ PDF, DOCX, DOC –¥–ª—è —Ä–µ–∂–∏–º–∞ '–†–∞–±–æ—Ç–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏'.")
        return
    os.makedirs(DOC_FOLDER, exist_ok=True)
    ts = int(time.time())
    file_path = os.path.join(DOC_FOLDER, f"{base}_{ts}.{ext}")
    new_file = await context.bot.get_file(document.file_id)
    await new_file.download_to_drive(file_path)

    if ext == "pdf":
        key = _pdf_job_key(document)
        if _pdf_job_is_running(context, key):
            return
        sent = await update.message.reply_text("–ü—Ä–∏–Ω—è—Ç PDF. –í—ã–ø–æ–ª–Ω—è—é —Å–∂–∞—Ç–∏–µ –∏ OCR‚Ä¶")
        _pdf_job_start(context, key, sent.message_id)
        try:
            def _job(pdf_path: str):
                pdf_use = _compress_pdf_sync(pdf_path) if PDF_COMPRESS else pdf_path
                quick = extract_text_from_pdf(pdf_use) or ""
                if (not FORCE_OCR) and len(quick.strip()) > 50:
                    use_text = _normalize_confusables_ru(quick)
                else:
                    use_text = _ocr_pdf_to_text_sync(pdf_use, OCR_DPI, OCR_MAX_PAGES, POPPLER_PATH, TESS_LANG)
                use_text = postprocess(use_text)
                docx_path = _save_text_as_docx(use_text, f"{Path(fname).stem}_ocr")
                chunks = smart_split_text(use_text)
                return use_text, chunks, docx_path, pdf_use != pdf_path, pdf_use
            text, chunks, derived_docx, compressed, pdf_final = await asyncio.to_thread(_job, file_path)
            context.user_data.setdefault("work_docs", []).append({
                "name": fname, "path": file_path, "text": text or "", "chunks": chunks or [],
                "from_pdf": True, "docx_path": derived_docx, "pdf_processed": pdf_final,
            })
            try:
                await update.effective_chat.edit_message_text(
                    message_id=sent.message_id,
                    text=f"PDF –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç. {'–°–∂–∞—Ç–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ. ' if PDF_COMPRESS else ''}"
                         f"OCR: {'—É—Å–ø–µ—Ö' if (text or '').strip() else '–Ω–µ —É–¥–∞–ª–æ—Å—å (–∏—Å–ø–æ–ª—å–∑—É—é –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç)'}."
                )
            except Exception:
                await update.message.reply_text(
                    f"PDF –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç. {'–°–∂–∞—Ç–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ. ' if PDF_COMPRESS else ''}"
                    f"OCR: {'—É—Å–ø–µ—Ö' if (text or '').strip() else '–Ω–µ —É–¥–∞–ª–æ—Å—å (–∏—Å–ø–æ–ª—å–∑—É—é –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç)'}."
                )
        finally:
            _pdf_job_finish(context, key)
        return
    # DOC / DOCX
    if ext == "docx":
        text = extract_text_from_docx(file_path)
    else:
        text = extract_text_from_doc(file_path)

    if not (text or "").strip():
        await update.message.reply_text("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —ç—Ç–æ –Ω–µ –ø—É—Å—Ç–æ–π/–∑–∞—â–∏—â—ë–Ω–Ω—ã–π —Ñ–∞–π–ª.")
        return
    chunks = smart_split_text(text)
    context.user_data.setdefault("work_docs", []).append({"name": fname, "path": file_path, "text": text, "chunks": chunks})
    await update.message.reply_text(f"–§–∞–π–ª –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç: {fname}. –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Å–µ—Å—Å–∏–∏: {len(context.user_data['work_docs'])}.")

async def handle_file(update: Update, context: Any):
    context.user_data.setdefault("work_docs", [])
    document: Document = update.message.document
    raw = document.file_name or "file"
    fname = Path(raw).name
    base, ext = os.path.splitext(fname)
    ext = _normalize_ext(ext.lower().lstrip("."))
    if ext not in ("pdf", "docx", "txt", "doc"):
        await update.message.reply_text("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ PDF, DOCX, DOC (–∏ TXT –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏).")
        return
    if context.user_data.get("mode") == "work":
        await _work_handle_file(update, context, document)
        return
    os.makedirs(DOC_FOLDER, exist_ok=True)
    ts = int(time.time())
    file_path = os.path.join(DOC_FOLDER, f"{base}_{ts}.{ext}")
    new_file = await context.bot.get_file(document.file_id)
    await new_file.download_to_drive(file_path)

    # Pre-calc hash for dedup
    try:
        file_hash = sha256_file(file_path)
    except Exception:
        file_hash = None

    manifest = load_manifest()
    if file_hash and file_hash in manifest.get("hashes", {}):
        await update.message.reply_text("–≠—Ç–æ—Ç —Ñ–∞–π–ª —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω —Ä–∞–Ω–µ–µ. –ú–æ–∂–µ—Ç–µ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã.")
        return
    # start indexing once; dedup by file_unique_id + hash
    key = _index_job_key(document.file_unique_id, file_hash)
    if _idx_is_running(context, key):
        return
    progress = await update.message.reply_text("–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–∞—á–∞–ª–∞—Å—å‚Ä¶")
    _idx_start(context, key, progress.message_id)

    try:
        def _index_job():
            try:
                added, total = index_file(file_path)
                return (True, added, total, None)
            except Exception as e:
                return (False, 0, 0, repr(e))
        ok, added, total, err = await asyncio.to_thread(_index_job)

        if ok:
            if added == 0:
                text = ("–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω, –Ω–æ —Ç–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á—ë–Ω. –í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ —Å–∫–∞–Ω –±–µ–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–ª–æ—è.\n"
                        "–ü—Ä–∏—à–ª–∏—Ç–µ DOCX/TXT –∏–ª–∏ PDF —Å —Ç–µ–∫—Å—Ç–æ–º, –ª–∏–±–æ –≤–∫–ª—é—á–∏—Ç–µ OCR (tesseract+poppler/Docker).")
            else:
                if file_hash:
                    manifest.setdefault("hashes", {})[file_hash] = {"fname": os.path.basename(file_path), "time": int(time.time())}
                    save_manifest(manifest)
                text = f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –î–æ–±–∞–≤–ª–µ–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {added}. –í—Å–µ–≥–æ: {total}. –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã."
        else:
            text = f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {err}"

        try:
            await update.effective_chat.edit_message_text(message_id=progress.message_id, text=text)
        except Exception:
            await update.message.reply_text(text)
    finally:
        _idx_finish(context, key)

async def _work_retrieve(docs: list, query: str, k: int = 6) -> list[str]:
    if not docs: return []
    all_chunks = [c for d in docs for c in d.get("chunks", [])]
    if not all_chunks: return []
    import numpy as np
    try:
        q = get_embedding(query)
    except Exception:
        return all_chunks[:k]
    embs = []
    for c in all_chunks:
        try: embs.append(get_embedding(c))
        except Exception: embs.append(None)
    scored = []
    for c, e in zip(all_chunks, embs):
        if e is None: continue
        denom = (np.linalg.norm(q) * np.linalg.norm(e)) or 1.0
        scored.append((float(q @ e / denom), c))
    scored.sort(key=lambda x: x[0], reverse=True)
    return [c for _, c in scored[:k]] if scored else all_chunks[:k]

async def _reply_with_docx(update: Update, title: str, content: str, base_name: str):
    safe_base = re.sub(r"[^A-Za-z–ê-–Ø–∞-—è0-9_.-]+", "_", base_name)[:60] or "report"
    filename = f"{int(time.time())}_{safe_base}.docx"
    path = _save_docx_report(filename, title, content)
    with open(path, "rb") as f:
        await update.message.reply_document(InputFile(f, filename=filename))


def _get_last_pdf_doc(context) -> dict | None:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ work_docs/general —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .pdf"""
    docs = (context.user_data.get("work_docs") or []) + (context.user_data.get("docs") or [])
    for rec in reversed(docs):
        name = rec.get("name") or ""
        if name.lower().endswith(".pdf"):
            return rec
    return None



async def pdf_to_txt(update: Update, context: Any):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π PDF –≤ TXT –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ñ–∞–π–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é."""
    rec = _get_last_pdf_doc(context)
    if not rec:
        await update.message.reply_text("–ù–µ –Ω–∞–π–¥–µ–Ω –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π PDF. –ü—Ä–∏—à–ª–∏—Ç–µ PDF-—Ñ–∞–π–ª –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ.")
        return
    text = (rec.get("text") or "").strip()
    if len(text) < 10:
        try:
            pdf_path = rec.get("path") or rec.get("pdf_processed") or ""
            if not pdf_path:
                await update.message.reply_text("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—É—Ç—å –∫ PDF. –ü—Ä–∏—à–ª–∏—Ç–µ —Ñ–∞–π–ª –µ—â—ë —Ä–∞–∑.")
                return
            from app.services.extract import extract_text_from_pdf
            text = extract_text_from_pdf(pdf_path)
        except Exception as e:
            await update.message.reply_text(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e!r}")
            return
    try:
        text = postprocess(text or "")
    except Exception:
        pass
    if not (text or "").strip():
        await update.message.reply_text("–¢–µ–∫—Å—Ç –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ PDF —á–∏—Ç–∞–±–µ–ª–µ–Ω.")
        return
    safe_base = re.sub(r"[^A-Za-z–ê-–Ø–∞-—è0-9_.-]+", "_", (rec.get("name") or "document"))[:60]
    filename = f"{int(time.time())}_{safe_base}.txt"
    import tempfile, os
    with tempfile.NamedTemporaryFile(mode="w+", suffix=".txt", delete=False, encoding="utf-8") as tmp:
        tmp.write(text)
        tmp.flush()
        tmp_path = tmp.name
    try:
        with open(tmp_path, "rb") as f:
            await update.message.reply_document(InputFile(f, filename=filename), caption="–ì–æ—Ç–æ–≤–æ: PDF ‚Üí TXT")
    finally:
        try:
            os.remove(tmp_path)
        except Exception:
            pass


async def doc_summary(update: Update, context: Any):
    docs = context.user_data.get("work_docs") or []
    if not docs:
        await update.message.reply_text("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –ü—Ä–∏—à–ª–∏—Ç–µ DOC/DOCX/PDF –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ.")
        return
    last = docs[-1]
    retrieved = await _work_retrieve([last], "–∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ —Ç–∏–ø —Ä–∞–±–æ—Ç/—É—Å–ª—É–≥/–ø–æ—Å—Ç–∞–≤–∫–∏/–∞–≥–µ–Ω—Ç—Å–∫–∏—Ö/–∞—Ä–µ–Ω–¥—ã", k=12)
    prompt = (
        "–¢—ã ‚Äî —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ù–∞ –æ—Å–Ω–æ–≤–µ –ö–û–ù–¢–ï–ö–°–¢–ê —Å–æ—Å—Ç–∞–≤—å –∫—Ä–∞—Ç–∫–æ–µ, –Ω–æ —Ç–æ—á–Ω–æ–µ —Ä–µ–∑—é–º–µ –¥–æ–≥–æ–≤–æ—Ä–∞.\n"
        "–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ:\n"
        "‚Ä¢ –û–ø—Ä–µ–¥–µ–ª–∏ –¢–ò–ü –ø—Ä–∞–≤–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–º–µ—Ç: —á—Ç–æ –∏–º–µ–Ω–Ω–æ –¥–µ–ª–∞–µ—Ç –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å –¥–ª—è –∑–∞–∫–∞–∑—á–∏–∫–∞ (–≤—ã–±–µ—Ä–∏ –∏–∑: –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ä–∞–±–æ—Ç, –æ–∫–∞–∑–∞–Ω–∏–µ —É—Å–ª—É–≥, –ø–æ—Å—Ç–∞–≤–∫–∞, –∞–≥–µ–Ω—Ç—Å–∫–∏–µ —É—Å–ª—É–≥–∏, –∞—Ä–µ–Ω–¥–∞), —É–∫–∞–∂–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞.\n"
        "‚Ä¢ –°—Ç–æ—Ä–æ–Ω—ã (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è), —Å—Ä–æ–∫, —Ü–µ–Ω–∞/–ø–æ—Ä—è–¥–æ–∫ –æ–ø–ª–∞—Ç—ã, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å/–Ω–µ—É—Å—Ç–æ–π–∫–∏, —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ, –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å/–ü–î–Ω, –ø—Ä–∞–≤–∞ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–µ—Å–ª–∏ –µ—Å—Ç—å).\n"
        "‚Ä¢ –ò–∑–±–µ–≥–∞–π –æ–±—â–∏—Ö —Ñ—Ä–∞–∑ ¬´–ø—Ä–µ–¥–º–µ—Ç –¥–æ–≥–æ–≤–æ—Ä–∞¬ª ‚Äî –ø–∏—à–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ.\n"
        "–§–æ—Ä–º–∞—Ç:\n"
        "‚Äî –¢–∏–ø/–ü—Ä–µ–¥–º–µ—Ç: ‚Ä¶\n‚Äî –°—Ç–æ—Ä–æ–Ω—ã: ‚Ä¶\n‚Äî –°—Ä–æ–∫: ‚Ä¶\n‚Äî –¶–µ–Ω–∞/–æ–ø–ª–∞—Ç–∞: ‚Ä¶\n‚Äî –û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å: ‚Ä¶\n‚Äî –†–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ: ‚Ä¶\n‚Äî –ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å/–ü–î–Ω: ‚Ä¶\n‚Äî –ü—Ä–∞–≤–∞ –Ω–∞ –†–ò–î: ‚Ä¶\n"
        "–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –±–µ–∑ –≤—ã–¥—É–º–æ–∫."
    )
    answer = generate_answer_with_gemini(prompt, retrieved) or ""
    if not answer.strip() or answer.strip().startswith("‚ö†Ô∏è"):
        answer = _fallback_summary(last["text"])
    await send_long(update, f"üìÑ –†–µ–∑—é–º–µ –ø–æ: {last['name']}\n\n{answer}")
    await _reply_with_docx(update, f"–†–µ–∑—é–º–µ: {last['name']}", answer, f"summary_{Path(last['name']).stem}")

async def doc_check(update: Update, context: Any):
    docs = context.user_data.get("work_docs") or []
    if not docs:
        await update.message.reply_text("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª.")
        return
    last = docs[-1]
    retrieved = await _work_retrieve([last], "–ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–≥–æ–≤–æ—Ä–∞ –Ω–∞ –æ—à–∏–±–∫–∏/—Ä–∏—Å–∫–∏/–ø—Ä–æ–±–µ–ª—ã/–Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è", k=16)
    prompt = (
        "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –¥–æ–≥–æ–≤–æ—Ä–Ω–æ–π —é—Ä–∏—Å—Ç. –ü—Ä–æ–≤–µ—Ä—å –¥–æ–≥–æ–≤–æ—Ä –Ω–∞ –æ—à–∏–±–∫–∏ –∏ —Ä–∏—Å–∫–∏ —Å—Ç—Ä–æ–≥–æ –ø–æ –ö–û–ù–¢–ï–ö–°–¢–£.\n"
        "–£–∫–∞–∂–∏: —Å–ø–æ—Ä–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ —É—Å–ª–æ–≤–∏—è, –ø—Ä–æ–±–µ–ª—ã (–Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç —É—Å–ª–æ–≤–∏–π), –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã —Ä–∞–∑–¥–µ–ª–æ–≤,\n"
        "—Ä–∏—Å–∫–∏ –ø–æ —Ü–µ–Ω–µ/—Å—Ä–æ–∫–∞–º/–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏/—Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—é/–†–ò–î/–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏/–ü–î–Ω –∏ –¥–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∞–≤–∫–∏ (–∫–∞–∫ –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å –ø—É–Ω–∫—Ç—ã).\n"
        "–°—Ç—Ä—É–∫—Ç—É—Ä–∞:\n1) –ö—Ä–∞—Ç–∫–∏–π –∏—Ç–æ–≥ —Ä–∏—Å–∫–æ–≤ (3‚Äì6 –ø—É–Ω–∫—Ç–æ–≤)\n2) –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º —Å —Ü–∏—Ç–∞—Ç–∞–º–∏\n3) –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–æ–∫ (bullet list)"
    )
    answer = generate_answer_with_gemini(prompt, retrieved) or ""
    if not answer.strip() or answer.strip().startswith("‚ö†Ô∏è"):
        answer = _fallback_check(last["text"])
    await send_long(update, f"üîé –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–≥–æ–≤–æ—Ä–∞: {last['name']}\n\n{answer}")
    await _reply_with_docx(update, f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–≥–æ–≤–æ—Ä–∞: {last['name']}", answer, f"check_{Path(last['name']).stem}")




async def doc_compare(update, context):
    docs = context.user_data.get("work_docs") or []
    if len(docs) < 2:
        await update.message.reply_text("–ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º –¥–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –µ—â—ë –æ–¥–∏–Ω –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ /doc_compare.")
        return
    a, b = docs[-2], docs[-1]
    text_a = a.get("text") or ""
    text_b = b.get("text") or ""

    raw_pairs = _pair_sentences(text_a, text_b)
    logging.getLogger("semantic-bot").info("[Gemini] call site: got %s raw pairs before filtering", len(raw_pairs))
    try:
        filtered = await _gemini_semantic_filter_sentence_pairs(raw_pairs, api_key=None)
    except Exception as e:
        logging.getLogger("semantic-bot").exception("[Gemini] filtering call failed ‚Äî using heuristic: %r", e)
        filtered = [(o, r) for (o, r) in raw_pairs if _heuristic_semantic_diff(o, r)]

    filename = f"{int(time.time())}_changes_{Path(a.get('name','original')).stem}_to_{Path(b.get('name','recognized')).stem}.docx"
    path = _save_compare_report_with_adapter(
        filtered_pairs=filtered,
        filename=filename,
        a_name=a.get('name','original.docx'),
        b_name=b.get('name','recognized.docx'),
        text_a=text_a,
        text_b=text_b,
    )

    try:
        with open(path, "rb") as f:
            await update.message.reply_document(InputFile(f, filename=os.path.basename(path)))
    except Exception:
        await update.message.reply_text(f"–§–∞–π–ª –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: {path}")


async def doc_clear(update: Update, context: Any):
    docs = context.user_data.get("work_docs") or []
    deleted_count = 0
    for d in docs:
        for key in ("path", "docx_path", "pdf_processed"):
            p = d.get(key)
            if p and os.path.exists(p):
                try:
                    os.remove(p); deleted_count += 1
                except Exception:
                    pass
    context.user_data["work_docs"] = []
    await update.message.reply_text(f"–û—á–∏—â–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {deleted_count}. –ö–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–ø–µ—Ä—å –ø—É—Å—Ç.")

async def handle_text(update: Update, context: Any):
    user_query = (update.message.text or "").strip()
    mode = context.user_data.get("mode", "docs")

    if user_query == AI_LABEL:
        await ai_mode(update, context); return
    if user_query == DOCS_LABEL:
        await docs_mode(update, context); return
    if user_query == TM_LABEL:
        await tm_mode(update, context); return
    if user_query == WORK_LABEL:
        await work_mode(update, context); return

    if mode == "ai":
        uid = update.effective_user.id
        if not is_admin(uid):
            used = get_usage(uid)
            if used >= DAILY_FREE_LIMIT:
                await update.message.reply_text(
                    "–î–æ—Å—Ç–∏–≥–Ω—É—Ç –¥–Ω–µ–≤–Ω–æ–π –ª–∏–º–∏—Ç –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –æ–±—Ä–∞—â–µ–Ω–∏–π –∫ –ò–ò (10). –í–æ–∑–≤—Ä–∞—â–∞–π—Ç–µ—Å—å –∑–∞–≤—Ç—Ä–∞ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.",
                    reply_markup=MAIN_KB,
                )
                return
        def _ai_job():
            try:
                return generate_direct_ai_answer(user_query)
            except Exception as e:
                return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {repr(e)}"
        answer = await asyncio.to_thread(_ai_job)
        if answer and not is_admin(uid):
            inc_usage(uid)
        await send_long(update, answer)
        if not is_admin(uid):
            left = max(0, DAILY_FREE_LIMIT - get_usage(uid))
            await update.message.reply_text(f"–û—Å—Ç–∞—Ç–æ–∫ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è: {left}.")
        return
    if mode == "work":
        docs = context.user_data.get("work_docs") or []
        if not docs:
            await update.message.reply_text("–ü–æ–∫–∞ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –ü—Ä–∏—à–ª–∏—Ç–µ DOC/DOCX/PDF.")
            return
        retrieved = await _work_retrieve(docs, user_query, k=RETRIEVAL_K)
        answer = generate_answer_with_gemini(user_query, retrieved) or ""
        if not answer.strip() or answer.strip().startswith("‚ö†Ô∏è"):
            answer = "–ö—Ä–∞—Ç–∫–∞—è —Å–ø—Ä–∞–≤–∫–∞ –ø–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º:\n" + _fallback_summary("\n\n".join(retrieved) if retrieved else docs[-1]["text"])
        await send_long(update, answer)
        return
    if mode == "tm":
        low = user_query.lower()
        if low.startswith("/tm_reg"):
            await tm_cmd_reg(update, context); return
        if low.startswith("/tm_exp"):
            await tm_cmd_exp(update, context); return
        await tm_handle_text(update, context)
        return
    if not (os.path.exists(INDEX_FILE) and os.path.exists(TEXTS_FILE)):
        await update.message.reply_text("–ù–µ—Ç –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª.")
        return
    def _answer_job():
        try:
            chunks = retrieve_chunks(user_query, k=RETRIEVAL_K)
            return generate_answer_with_gemini(user_query, chunks)
        except Exception as e:
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {repr(e)}"

    answer = await asyncio.to_thread(_answer_job)
    if not answer:
        await update.message.reply_text("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç.")
    else:
        await send_long(update, answer)

async def error_handler(update: object, context: Any) -> None:
    logger.exception("Unhandled error while processing update: %s", update)

from telegram import InputFile
from app.config import INDEX_FILE, TEXTS_FILE, DOCMETA_FILE

async def download_index(update: Update, context: Any):
    # –¢–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–æ–≤
    if not is_admin(update.effective_user.id):
        await update.message.reply_text("–ö–æ–º–∞–Ω–¥–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞–º.")
        return
    try:
        path = INDEX_FILE
        await _send_file_safely(update, path, "index.faiss", "FAISS-–∏–Ω–¥–µ–∫—Å")
    except Exception as e:
        await update.message.reply_text(f"–û—à–∏–±–∫–∞: {e}")


async def download_texts(update: Update, context: Any):
    # –¢–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–æ–≤
    if not is_admin(update.effective_user.id):
        await update.message.reply_text("–ö–æ–º–∞–Ω–¥–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞–º.")
        return
    try:
        path = TEXTS_FILE
        await _send_file_safely(update, path, "texts.pkl", "–ß–∞–Ω–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤")
    except Exception as e:
        await update.message.reply_text(f"–û—à–∏–±–∫–∞: {e}")


async def download_meta(update: Update, context: Any):
    # –¢–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–æ–≤
    if not is_admin(update.effective_user.id):
        await update.message.reply_text("–ö–æ–º–∞–Ω–¥–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞–º.")
        return
    try:
        path = DOCMETA_FILE
        await _send_file_safely(update, path, "docmeta.json", "–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    except Exception as e:
        await update.message.reply_text(f"–û—à–∏–±–∫–∞: {e}")


async def _count_tokens_safe(model, text: str) -> int:
    try:
        ct = await asyncio.to_thread(model.count_tokens, text)
        for attr in ("total_tokens", "total_token_count", "token_count"):
            if hasattr(ct, attr):
                val = getattr(ct, attr)
                try:
                    return int(getattr(val, "value", val))
                except Exception:
                    try:
                        return int(val)
                    except Exception:
                        pass
        return 0
    except Exception:
        return (len(text) + 3) // 4

def _batch_pairs_by_limits(items: List[Tuple[str, str]], max_pairs: int = 60, max_chars: int = 18000):
    batch, cur_chars = [], 0
    for (o, r) in items:
        o_t, r_t = _truncate(o), _truncate(r)
        payload = json.dumps({"original": o_t, "recognized": r_t}, ensure_ascii=False)
        if (len(batch) >= max_pairs) or (cur_chars + len(payload) > max_chars):
            if batch:
                yield batch
            batch, cur_chars = [], 0
        batch.append((o_t, r_t))
        cur_chars += len(payload)
    if batch:
        yield batch

def _build_prompt(items: List[Tuple[str, str]]) -> str:
    return (
        """–¢—ã ‚Äî —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Ä–µ–¥–∞–∫—Ç–æ—Ä. –î–∞–Ω—ã –ø–∞—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞.
- ORIGINAL ‚Äî –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ DOCX
- RECOGNIZED ‚Äî –∏–∑ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ PDF (OCR-–æ—à–∏–±–∫–∏ –≤–æ–∑–º–æ–∂–Ω—ã)

–ù—É–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –¢–û–õ–¨–ö–û –ø–∞—Ä—ã —Å–æ —Å–º—ã—Å–ª–æ–≤—ã–º –æ—Ç–ª–∏—á–∏–µ–º (—Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è).
–ò–≥–Ω–æ—Ä–∏—Ä—É–π —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø—Ä–æ–±–µ–ª–∞—Ö, —Ä–µ–≥–∏—Å—Ç—Ä–µ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏.
–ï—Å–ª–∏ –≤ –ø–∞—Ä–µ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –ß–ò–°–õ–ê –∏–ª–∏ –¥–∞—Ç—ã ‚Äî —ç—Ç–æ —Å–º—ã—Å–ª–æ–≤–æ–µ –æ—Ç–ª–∏—á–∏–µ.

–í–µ—Ä–Ω–∏ –ß–ò–°–¢–´–ô JSON-–º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤ (–±–µ–∑ –ø–æ–¥—Å–∫–∞–∑–æ–∫/–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –ë–ï–ó Markdown-–∫–æ–¥–∞):
[
  {"original": "...", "recognized": "..."},
  ...
]

PAIRS:
"""
        + json.dumps([{"original": o, "recognized": r} for (o, r) in items], ensure_ascii=False)
    )


async def _gemini_semantic_filter_sentence_pairs(pairs, api_key: str | None):
    log = logging.getLogger("semantic-bot")
    log.info("[Gemini] entry: pairs=%s", len(pairs) if pairs else 0)

    if not pairs:
        log.info("[Gemini] no pairs ‚Äî nothing to filter")
        return []

    key = api_key or os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
    if not key:
        log.info("[Gemini] API key missing ‚Äî skip filtering; using heuristic")
        return [(o, r) for (o, r) in pairs if _heuristic_semantic_diff(o, r)]

    try:
        import google.generativeai as genai
        genai.configure(api_key=key)
        model = genai.GenerativeModel("gemini-1.5-flash")
    except Exception as e:
        log.exception("[Gemini] init failed: %r ‚Äî using heuristic", e)
        return [(o, r) for (o, r) in pairs if _heuristic_semantic_diff(o, r)]

    # smaller batches
    def _batch_pairs_by_limits(items, max_pairs: int = 40, max_chars: int = 12000):
        batch, cur_chars = [], 0
        for (o, r) in items:
            o_t, r_t = (o or "").strip(), (r or "").strip()
            payload = json.dumps({"original": o_t, "recognized": r_t}, ensure_ascii=False)
            if (len(batch) >= max_pairs) or (cur_chars + len(payload) > max_chars):
                if batch:
                    yield batch
                batch, cur_chars = [], 0
            batch.append((o_t, r_t))
            cur_chars += len(payload)
        if batch:
            yield batch

    batches = list(_batch_pairs_by_limits(pairs, max_pairs=40, max_chars=12000))
    log.info("[Gemini] filtering enabled, total pairs: %s, batches: %s", len(pairs), len(batches))

    filtered_total = []
    disable_gemini_for_run = False

    for i, batch in enumerate(batches):
        if disable_gemini_for_run:
            filtered_total.extend([(o, r) for (o, r) in batch if _heuristic_semantic_diff(o, r)])
            continue

        prompt = _build_prompt(batch)
        try:
            tok = await _count_tokens_safe(model, prompt)
            if tok > 30000 and len(batch) > 1:
                mid = len(batch) // 2
                batches[i:i+1] = [batch[:mid], batch[mid:]]
                log.info("[Gemini] split batch %s due to token estimate %s", i+1, tok)
                continue
        except Exception:
            pass

        attempt, last_err = 0, None
        while attempt < 2 and not disable_gemini_for_run:
            attempt += 1
            try:
                log.info("[Gemini] batch %s/%s, size: %s (attempt %s)", i+1, len(batches), len(batch), attempt)
                resp = await asyncio.to_thread(
                    lambda: model.generate_content(
                        contents=[{"role": "user", "parts": [{"text": prompt}]}],
                        generation_config={
                            "response_mime_type": "application/json",
                            "temperature": 0.0,
                            "candidate_count": 1,
                        },
                    )
                )
                raw = getattr(resp, "text", None) or getattr(resp, "output_text", None) or ""
                data = _extract_json_array(raw)
                if not isinstance(data, list):
                    raise ValueError("Gemini did not return a JSON array")

                kept = 0
                for item in data:
                    o = (item.get("original") or "").strip() if isinstance(item, dict) else ""
                    r = (item.get("recognized") or "").strip() if isinstance(item, dict) else ""
                    if o and r and _heuristic_semantic_diff(o, r):
                        filtered_total.append((o, r)); kept += 1
                log.info("[Gemini] batch %s kept: %s/%s", i+1, kept, len(batch))
                break
            except Exception as e:
                last_err = e
                msg = f"{e.__class__.__name__}: {e}"
                log.warning("[Gemini] batch %s failed: %s", i+1, msg)
                if ("ResourceExhausted" in e.__class__.__name__) or ("quota" in str(e).lower()):
                    log.error("[Gemini] quota hit ‚Äî disabling Gemini for this run, using heuristic for remaining batches")
                    disable_gemini_for_run = True
                    break
                await asyncio.sleep(0.6 * attempt)

        if last_err and (attempt >= 2 or disable_gemini_for_run):
            log.error("[Gemini] batch %s fallback to heuristic due to persistent error: %r", i+1, last_err)
            filtered_total.extend([(o, r) for (o, r) in batch if _heuristic_semantic_diff(o, r)])

    log.info("[Gemini] filtered pairs total: %s (from %s)", len(filtered_total), len(pairs))
    return filtered_total

def _split_sentences(text: str):
    text = (text or "").strip()
    if not text:
        return []
    parts = [p.strip() for p in _SENT_SPLIT_RE.split(text) if p.strip()]
    return parts

def _pair_sentences(a_text: str, b_text: str):
    a_s = _split_sentences(a_text)
    b_s = _split_sentences(b_text)
    sm = difflib.SequenceMatcher(a=a_s, b=b_s, autojunk=False)
    pairs = []
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag == "equal":
            continue
        if tag == "replace":
            for i in range(max(i2-i1, j2-j1)):
                o = a_s[i1+i] if i1+i < i2 else ""
                r = b_s[j1+i] if j1+i < j2 else ""
                if o or r:
                    pairs.append((o, r))
        elif tag == "delete":
            for i in range(i1, i2):
                pairs.append((a_s[i], ""))
        elif tag == "insert":
            for j in range(j1, j2):
                pairs.append(("", b_s[j]))
    return pairs

def _save_docx_semantic_pairs(filename: str, doc_name_a: str, doc_name_b: str, pairs):
    os.makedirs(DOC_FOLDER, exist_ok=True)
    path = os.path.join(DOC_FOLDER, filename)
    doc = DocxDocument()
    doc.add_heading(f"–°–º—ã—Å–ª–æ–≤—ã–µ –æ—Ç–ª–∏—á–∏—è: {doc_name_a} ‚Üí {doc_name_b}", level=1)
    if not pairs:
        doc.add_paragraph("–°–º—ã—Å–ª–æ–≤—ã—Ö –æ—Ç–ª–∏—á–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        doc.save(path); return path

    table = doc.add_table(rows=1, cols=2)
    hdr_cells = table.rows[0].cells
    hdr_cells[0].text = "–û—Ä–∏–≥–∏–Ω–∞–ª (DOCX)"
    hdr_cells[1].text = "–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π (OCR)"
    for (o, r) in pairs:
        row = table.add_row().cells
        row[0].text = o or "‚Äî"
        row[1].text = r or "‚Äî"
    doc.save(path)
    return path



def _extract_json_array(raw: str):
    if not raw:
        return []
    s = raw.strip()
    if s.startswith("```"):
        s = s.strip("`").strip()
        if s.lower().startswith("json"):
            s = s[4:].strip()
    try:
        data = json.loads(s)
        return data if isinstance(data, list) else []
    except Exception:
        pass
    l = s.find("["); r = s.rfind("]")
    if l != -1 and r != -1 and r > l:
        chunk = s[l:r+1].strip()
        try:
            data = json.loads(chunk)
            return data if isinstance(data, list) else []
        except Exception:
            parts = re.split(r"\]\s*[\r\n]+\s*\[", chunk.strip()[1:-1])
            items = []
            for part in parts:
                pj = "[" + part.strip() + "]"
                try:
                    items.extend(json.loads(pj))
                except Exception:
                    pass
            if items:
                return items
    objs = re.findall(r"\{[^{}]*\}", s, flags=re.DOTALL)
    if objs:
        try:
            return [json.loads(o) for o in objs]
        except Exception:
            pass
    return []





def _save_compare_report_with_adapter(filtered_pairs, filename, a_name, b_name, text_a, text_b):
    """
    Universal caller for project _save_docx_compare_tables with signature detection.
    Tries keyword invocation via inspect.signature; then tries several positional orders.
    Fallbacks to DOCX/TSV writer.
    """
    if "_save_docx_compare_tables" in globals():
        fn = _save_docx_compare_tables
        # Keyword attempt
        try:
            sig = inspect.signature(fn)
            params = list(sig.parameters.keys())
            kwargs = {}
            mapping = {
                "pairs": filtered_pairs,
                "filtered_pairs": filtered_pairs,
                "filename": filename,
                "doc_name_a": a_name,
                "doc_name_b": b_name,
                "text_a": text_a,
                "text_b": text_b,
            }
            for p in params:
                if p in mapping:
                    kwargs[p] = mapping[p]
            if kwargs and ("filename" in kwargs and ("pairs" in kwargs or "filtered_pairs" in kwargs)):
                return fn(**kwargs)
        except Exception:
            logging.getLogger("semantic-bot").exception("Project _save_docx_compare_tables kwargs call failed; try positional")

        # Positional attempts
        orders = [
            (filename, filtered_pairs, b_name, text_a, text_b),
            (filename, filtered_pairs, a_name, b_name, text_a, text_b),
            (filtered_pairs, filename, b_name, text_a, text_b),
            (filtered_pairs, filename, a_name, b_name, text_a, text_b),
        ]
        for args in orders:
            try:
                return fn(*args)
            except TypeError:
                continue
            except Exception:
                logging.getLogger("semantic-bot").exception("Project _save_docx_compare_tables positional attempt failed; trying next")
        logging.getLogger("semantic-bot").error("Project _save_docx_compare_tables signature mismatch ‚Äî using fallback writer")

    # Fallback writer
    try:
        from docx import Document as DocxDocument
        os.makedirs("generated_reports", exist_ok=True)
        path = os.path.join("generated_reports", filename)
        doc = DocxDocument()
        doc.add_heading(f"–°–º—ã—Å–ª–æ–≤—ã–µ –æ—Ç–ª–∏—á–∏—è: {a_name} ‚Üí {b_name}", level=1)
        if not filtered_pairs:
            doc.add_paragraph("–°–º—ã—Å–ª–æ–≤—ã—Ö –æ—Ç–ª–∏—á–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        else:
            table = doc.add_table(rows=1, cols=2)
            table.rows[0].cells[0].text = "–û—Ä–∏–≥–∏–Ω–∞–ª (DOCX)"
            table.rows[0].cells[1].text = "–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π (OCR)"
            for o, r in filtered_pairs:
                row = table.add_row().cells
                row[0].text = o or "‚Äî"
                row[1].text = r or "‚Äî"
        doc.save(path)
        return path
    except Exception:
        os.makedirs("generated_reports", exist_ok=True)
        path = os.path.join("generated_reports", filename.replace(".docx", ".tsv"))
        with open(path, "w", encoding="utf-8") as f:
            f.write("–û—Ä–∏–≥–∏–Ω–∞–ª\t–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π\n")
            for o, r in filtered_pairs:
                line_o = (o or "").replace("\t", " ")
                line_r = (r or "").replace("\t", " ")
                f.write(line_o + "\t" + line_r + "\n")
        return path


from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters
from app.config import TELEGRAM_BOT_TOKEN

def build_application():
    app = ApplicationBuilder().token(TELEGRAM_BOT_TOKEN).build()

    # === –∫–æ–º–∞–Ω–¥—ã ===
    try:
        app.add_handler(CommandHandler("start", start))
    except Exception:
        pass
    try:
        app.add_handler(CommandHandler("help", help_command))
    except Exception:
        pass
    try:
        app.add_handler(CommandHandler("download_index", download_index))
        app.add_handler(CommandHandler("download_texts", download_texts))
        app.add_handler(CommandHandler("download_meta", download_meta))
    except Exception:
        pass

    try:
        app.add_handler(CommandHandler("tm_reg", tm_cmd_reg))
        app.add_handler(CommandHandler("tm_exp", tm_cmd_exp))
    except Exception:
       pass

# === –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª–∏ —Ä–µ–∂–∏–º–æ–≤ ===
    try:
        app.add_handler(MessageHandler(filters.TEXT & filters.Regex(f"^{re.escape(TM_LABEL)}$"), tm_mode))
        app.add_handler(MessageHandler(filters.TEXT & filters.Regex(f"^{re.escape(WORK_LABEL)}$"), work_mode))
        app.add_handler(MessageHandler(filters.TEXT & filters.Regex(f"^{re.escape(DOCS_LABEL)}$"), docs_mode))
        app.add_handler(MessageHandler(filters.TEXT & filters.Regex(f"^{re.escape(AI_LABEL)}$"), ai_mode))
    except Exception:
        pass

    # === –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ —Ñ–∞–π–ª–æ–≤ ===
    try:
        app.add_handler(MessageHandler(filters.Document.ALL, handle_file))
        app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text))
    except Exception:
        pass

    # === –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ ===
    try:
        app.add_error_handler(error_handler)
    except Exception:
        pass

    return app

__all__ = ["build_application"]
